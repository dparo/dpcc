\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{microtype,etex,listings,color,parskip, graphicx, float}
\usepackage[margin=2cm]{geometry}
\usepackage[hidelinks]{hyperref}

\newcommand{\secref}[3][green]{\href{#2}{\color{#1}{#3}}}%
\newcommand{\urlref}[3][blue]{\href{#2}{\color{#1}{#3}}}%





\usepackage{wasysym}

\lstset{
    language=C,
    tabsize=2,
    showstringspaces=false,
    breaklines=true,
    basicstyle=\ttfamily,
    keywordstyle=\color[rgb]{0.1,0.3,0.7}\ttfamily,
    stringstyle=\color[rgb]{0.7,0.1,0.3}\ttfamily,
    commentstyle=\color[rgb]{0.3,0.4,0.3}\ttfamily,
    columns=fixed,
    numberstyle=\sffamily\scriptsize,
    backgroundcolor=\color[rgb]{0.95,0.95,0.95},
    frame=lines,
    framexleftmargin=5pt,
    numbers = left,
    numberstyle = \footnotesize,
}



\lstdefinelanguage{JS}{
    keywords={ var, typeof, new, true, false, catch,
        function, return, null, catch, switch, let, var, if, of, in,
    for, while, do, else, case, break, const},
    keywordstyle=\color{blue}\bfseries,
    ndkeywords={class, export, boolean, throw, implements, import, this},
    ndkeywordstyle=\color{darkgray}\bfseries,
    identifierstyle=\color{black},
    sensitive=false,
    comment=[l]{//},
    morecomment=[s]{/*}{*/},
    commentstyle=\color[rgb]{0.3,0.4,0.3}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    morestring=[b]',
    morestring=[b]"
}

\lstset{
    language=JS,
    extendedchars=true,
    basicstyle=\footnotesize\ttfamily,
    showstringspaces=false,
    showspaces=false,
    numbers=left,
    columns=flexible,
    numberstyle=\tiny,
    numbersep=9pt,
    tabsize=2,
    breaklines=true,
    showtabs=false,
    captionpos=b,
}

\lstdefinelanguage{DPL}{
    keywords={ fn, print, int, float, bool, var, typeof, new, true, false, catch,
        function, return, null, catch, switch, let, var, if, of, in,
    for, while, do, else, case, break, const},
    keywordstyle=\color{blue}\bfseries,
    ndkeywords={class, export, boolean, throw, implements, import, this},
    ndkeywordstyle=\color{darkgray}\bfseries,
    identifierstyle=\color{black},
    sensitive=false,
    comment=[l]{//},
    morecomment=[s]{/*}{*/},
    commentstyle=\color[rgb]{0.3,0.4,0.3}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    morestring=[b]',
    morestring=[b]"
}

\lstset{
    language=DPL,
    extendedchars=true,
    basicstyle=\footnotesize\ttfamily,
    showstringspaces=false,
    showspaces=false,
    numbers=left,
    columns=flexible,
    numberstyle=\tiny,
    numbersep=9pt,
    tabsize=2,
    breaklines=true,
    showtabs=false,
    captionpos=b,
}

\begin{document}
\tableofcontents


\title{DPCC: DParo's Own C-Alike Compiler}
\author{Davide Paro}
\date{December 2020}

\maketitle

\section{Assignment Description}
This project is about an assignment for a course on \textbf{Compilers}
at the department of Computer Engineering Master Degree Padova (ITA).


The assignment consists in implementing a toy compiler for a toy language. More emphasis
is put on the implementation for the frontend side (input, lexing, parsing, type checking), while
the backend side is stubbed out by a simple 3AC \footnote{3AC: Three Address Code} Intermediate Code
generator.
We are free to design the syntax of this toy language however we like.

The assignment specs out the how the compiler should be composed.
We can in fact distinguish these macro components:

\begin{itemize}
\item \textbf{Input Stage} deals with the input byte stream that composes
    the source of the program.
\item \textbf{Lexer/Scanner} has the purpose of grouping characters (lexical analysis)
    together to compose compunded
    structures (called tokens). For the project assignment we can use \textbf{Flex} to aid in
    the implementation of the scanner.
\item \textbf{Parser} for performing the syntax analysis. It is what defines the look \& fell
    (grammar) of the language. For the project assignment we can use \textbf{Bison} to aid in the
    implementation of an LR parser.
\item \textbf{Intermediate Code Generator}. The ultimate purpose of a compiler is to produce something
    useful. In this project assignment we are not asked to implement a proper backend. Instead,
    we need to emit a 3AC representation of our input program. More in this later.
\end{itemize}

In particular the final Intermediate representation that we need to emit is based on Three Address Code (3AC),
that is, each statement can only have 1 operand at the left hand side of the assignment, and 2 operands at the right hand side
of the assignment, and an operator driving the operation that should be performed.

You can view more about 3AC at the following \urlref{https://en.wikipedia.org/wiki/Three-address_code}{Wikipedia link}.

In practice the emitted 3AC code is on itself a partially valid C program, it's only missing variable
declarations at the top for the temporary variables.


For the specification of the Intermdiate Code that is generated please refer to \hyperref[appendix_a]{appendix A}

So the project requires to produce this kind of 3AC / C hybrid. Control flow is allowed to be implemented
trough the usage of C labels and simple if conditional followed by a goto statement. Inside
the if conditional there can only be a single element composing the expression.

The assignment requires the following features from the programming langugage that we should develop:

\begin{itemize}
    \item Variables declaration, initialization and assignment
    \item Scoping. Variable names are reusable in different scopes. Variable
        shadowing may or may not warn/fail/pass depending on the design choices.
    \item Only 2 types of variables: integers, booleans
    \item Assignment statements, print statements, if statements, and at least 1 loop statement at our liking
    \item Handling of simple mathematical expressions that we can encounter in common programming
        languages: addition, subtraction, multiplication, division, modulo, etc \dots
    \item \textbf{Function definition, function calls, and custom user definable types are not required}
\end{itemize}

\clearpage

\section{DPL and dpcc: A quick peak at the language and at the compiler}

\textbf{DPL} and \textbf{DPCC} are respectively the \textbf{name of the language} and the
\textbf{name of the implemented compiler}. They are named after their author.

From now on \textbf{DPL} and \textbf{DPCC} will be used for brevity for refering to the language and to the compiler.
We will use \textbf{dpcc} (all lowercase) instead, to refer to the actual executable where the compiler lives.

That being said \textbf{DPCC} (all UPPERCASE) and \textbf{dpcc} (all lowercase) are mostly used interchangeably to refer to the same thing.


\subsection{DPL: Structure of the language}

\textbf{DPL} is mostly a C-alike compatible language, since it borrows most of its syntax and semantics.
It also borrows some syntax from \textbf{Rust} \&
\textbf{JS} especially in how the variables are declared (usage of the keyword \textbf{let}).

\textbf{Rust} is a modern system programming language with strong typing guarantees.
Among all the interesting features that Rust provides one of them is type deduction.
Thanks to Rust's strong typing guarantees and thanks to strong type deduction rules implemented
inside the \textbf{rustc} compiler, Rust
allows one to declare variables with a very low-weight syntax, similar
to a syntax provided from a typical dynamic language (for example JS).

\textbf{DPL}, like Rust, also have a very simple form of a type deduction system.
It is not even remotely close to the Rust type deduction system, but still it
allows the user of the language to not always need to specify the type of each variable in a declaration.
How this is achieved will be described in later sections.

In \textbf{DPL}:

\begin{itemize}
\item Spaces and newlines mostly do not matter, they simply introduce token boundaries.
\item Comments start with the double forward slash `\texttt{//}', and C-style multiline comments are
instead not supported (at the time of writing).
\end{itemize}



Here follows a chunk of \textbf{DPL} code to show off the syntax and some of the language features:

\begin{lstlisting}[language=DPL]
// Print statement with immediate C-style strings. C-style strings can only be used inside print statement
print("Hello world\n");

// Variable declaration and initialization
let a = 10;          // Integer Type deduced
let f = 10.0;        // Float type deduced
let b = false;       // Boolean type deduced

// Explicit types
let i: int = 0xffff & ~0xb00111;
let f: float = 10.0 + 20.0 ** 2;

// Immediate values can be printed
print(10);
print(30 + 4);

// Variables can be printed
print(i);       // Print integer
print(f);       // Print float

// Casting can be used to enforce type conversion
let myInt: int = int(10.00f);
let myFloat = float(0xFF);

// Type deduction
let b = (10 < 20);   // Boolean type is deduced
let f = 1 + 2.0f;    // Float type is deduced (the 1 is upcasted to a float)

// Scoping and restricting variable declarations to the current scope
{
    // Simple single dimension arrays declaration
    let buf_i: int[100];            // Known size integer array
    let buf_f: float[100];          // Known size float array

    // Integer array with deduced size from the RHS initializer list
    let buf = [ 10, 20, 30, 40, 50 ];
    let fs: float[] = [ 0.1, 0.2, 0.3, 0.4, 0.5 ];

    // Arrays can be printed
    print(buf);
}

let buf: int[100];

// Control flow
for (let i = 0; i < 100; i++) {
    buf[i] = i ** 2;

    if (buf[i] == 10) {
        print("buf[i] is 10!!!\n")
    }
    else if (buf[i] == 20) {
        print("buf[i] is 20!!!\n");
    }
    else {
        print("None of above\n");
    }
}
\end{lstlisting}

The cool thing about \textbf{DPL}, is that it is \textbf{almost a Javascript subset}.
That is one can simply copy the \textbf{DPL} code, strip the type information (if they're used
anywhere) by manual editing or automatically, and paste the same code into a browser console to evaluate as JS code.
With a couple modifications here and there (for example arrays with no initalizer list must be converted into a valid
JS array), one can
test if the compiler \textbf{dpcc} is producing the correct output by simply
evaluating the same code in a browser console.

This example shows how to convert \textbf{DPL} into \textbf{JS} by manual editing:

\begin{lstlisting}[language=DPL]
// This is a chunk of DPL code
let a = 10;                     // This is also valid JS code
let b = [ 10, 20, 30, 40 ];     // This is also valid JS code
let c: int = 10;                // Mostly valid JS code, remove the type info and the colon
let d: int[1024];               // Js arrays grow on demand automatically when touching elements.
                                // No need to specify neither type nor number of elements

print(d);                       // Valid JS code if function print were to be defined
\end{lstlisting}

Here's the equivalent JS code:

\begin{lstlisting}[language=JS]
// This is the equivalent Javascript
const print = console.log;      // Define it once at the top of the script

let a = 10;                     // Same as before
let b = [ 10, 20, 30, 40 ];     // Same as before
let c = 10;                     // Just strip the int type
let d = [];                     // Just initalizing with an empty array is enough

print(d);                       // Works because print is defined at the top
\end{lstlisting}

Most other \textbf{DPL} syntax and features, like code blocks, conditionals, loops \dots etc are valid JS code
thanks on how the grammar for \textbf{DPL} was defined.

For now \textbf{dpcc} supports only 5 types: \texttt{bool, int, float, string, bool[], int[], float[]}.
Only single dimensions array are for now supported. So arrays do not generalize to any number of dimensions.

Most of these types have full on semantics, meaning that the compiler can deduce
a type of an expression given the types of its operand. In some cases it can reject
the code if the operands of an expression have invalid types.
At the current time of writing, \texttt{string} types are quirky, meaning that
they don't have a full type tracking inside
the compiler like other types do.
The compiler still knows what a string is, and in it marks correctly \textbf{string literals} as
a \texttt{string} type, but strings undergo different semantics.
They cannot be assigned or operated on like a variable, but instead,
they can only be used as a parameter to the print statement.


\subsection{DPCC: Using the compiler}

The \textbf{dpcc} compiler is written in the \textbf{C} language. Unfortunately at the current time
of writing \textbf{dpcc} works only under Unix like operatins systems. The compiler has
been tested under Ubuntu 20.10, Ubuntu 20.04, and macOS 10.15. The
compiler was developed by his author using an Ubuntu 20.10 machine, while the other distro/OS
were tested thanks to Github Actions automated build-check cycles. Windows builds
failed due to MSVC rejecting the source code of \textbf{dpcc} cause it contains some GCC
extensions and some hard coded unix syscalls.
In short words \textbf{dpcc} can be only compiled with either GCC or CLANG compilers
and executed in a Unix/Posix compatible operating system.

If you would like to build the compiler yourself from scratch please refer to the \urlref{https://github.com/dparo/dpcc/wiki}{Project WIKI}\footnote{\urlref{https://github.com/dparo/dpcc}{Github Repo Link}}

The compiler can and should be invoked from the commandline. The \textbf{dpcc} executable
is self contained and doesn't reach for any implicit external asset and thus can be placed
anywhere in the system and invoked from anywhere.

From now on we assume the user has a fired up shell correctly \textbf{cd}-ed to the directory
holding the \textbf{dpcc} executable:

To call the compiler run the following command, which will print it's usage help message:

\begin{lstlisting}[language=Bash]
./dpcc
\end{lstlisting}

The \textbf{dpcc} compiler supports the specification of the \texttt{-o} flag
where applicable. This flag allows to override the default output location.

\textbf{dpcc} can work in 6 different modes: \textbf{lex, parse, 3ac, c, gcc, run}:

\begin{itemize}
    \item \texttt{./dpcc lex <input> [-o <out>]}: Lex the input and show the list of tokens composing the \textbf{DPL} source in either stdout or in the given file.
    \item \texttt{./dpcc parse <input> [-o <out>]}: Parse the program and produce a text representation of the AST (Abstract Syntax Tree) in either stdout or in the given file.
    \item \texttt{./dpcc 3ac <input> [-o <out>]}: Parse the program and perform additional type validations and type checking. If the program is still valid emit 3AC in either stdout or in the given file.
    \item \texttt{./dpcc c <input> [-o <out>]}: Same as 3AC but also emit preamble and postamble required  to promote 3AC to a  valid C program that can be compiled. The output is emitted in either stdout or in the given file.
    \item \texttt{./dpcc gcc <input> [-o <out>]}: Same as \texttt{`dpcc c'} but the generated C program is piped into GCC standard input and the final executable is either compiled in \texttt{a.out} or in the given filepath. This requires GCC to be in the path.
    \item \texttt{./dpcc run <input>}: Parse, typecheck, emit the C code, compile it and run it in one single command. The executable produced by GCC is outputted in a temp file (under \texttt{/tmp}), the temp executable is executed right away and then removed. The \texttt{-o} flag is ignored. This requires GCC to be in the path.
\end{itemize}

\texttt{Lex} and \texttt{parse} modes are mostly used for debugging and are not really that useful.
The \texttt{run} mode
is the most convenient mode since it takes care of everything. If the input program is valid and
you call \texttt{`./dpcc run'} on it you will see the output generated from you \textbf{DPL} program,
otherwise the compiler will complain with either warnings or errors.

\clearpage

\section{DPL Language Details}

Providing a full language specification is beyond the scope of this project report. In particular
this section will not list the entire grammar of the language. Thus, it is assumed
that the reader has a common basic programming language knowledge. It is also assumeed that he/she has some experience
with at least one C-alike language. If the reader satisfies these requirements, he/she can use
basic reasoning and code examples to deduce the specification of the language. Thus the purpose
of this section is to characterize some core major concepts that are distinguish
\textbf{DPL} from other languages and that not easily inferable:

\begin{itemize}
    \item Comments start with `\texttt{//}'
    \item Identifiers start with a letter, an underscore or an extened non ASCII UTF-8 character. After the first
        character an identifier can contain any alphanumerical character excluding spaces and any punctuation character.
    \item Strings are enclosed in \texttt{"} (double quotes) and can contain valid ASCII escape sequences like
        in any traditional C-derived language
    \item Print statement unlike in C are allowed to print any variable, and can deduce what should be printed
        based on the type of the variable that is passed.
    \item Most of the grammar is C-inspired, and in fact all control flow statements have
        the same syntax of any C derived language.
    \item The precedence of the operators are taken directly from the \urlref{https://en.cppreference.com/w/c/language/operator_precedence}{C precedence table}.
        The only \textbf{modification that DPL does differently than C} is that bitwise operators (\texttt{\&, |, \^})
        have higher precedence than the compare operators (\texttt{==, !=, \dots}). Most modern languages
        adopt this convenient change, because it makes the precedence of the bitwise operators behave in the same
        way that normal mathematical operators work (\texttt{=, +, -, <<, >>}).
        In fact also \textbf{the Rust language employes this same modification}.
    \item A \textbf{DPL} program starts either in 2 ways. The first more idiomatic way is to just start
        writing statements directly:
        \begin{lstlisting}[language=DPL]
let a = 10;
print(a + 20);
        \end{lstlisting}

        The other way is to wrap \textbf{all} the statements inside a main function.

        \begin{lstlisting}[language=DPL]
fn main() {
    let = 10;
    print(a + 20);
}
        \end{lstlisting}

    \textbf{Since DPL does not support functions yet} the main function is mostly ignored but it is still
    part of the grammar for consistency reason.
    \item \textbf{DPL} is a strongly typed language. Currently only 5 types are supported: \texttt{bool, int, float, string, bool[], int[], float[]}. Which
        as we talked about in previous sections \texttt{string} types behave a little bit different way.


    \item Code blocks are enclosed in braces `\texttt{\{ \dots \ \}}'. Each code block define a new scope where variables can
        be defined.
    \item \textbf{Braces in control flow statements} (\texttt{if, while, for, \dots}) \textbf{are always mandatory}. This is different from C where the braces are not mandatory. This change was done to simplify the grammar but also to avoid ambiguity and to make the code more robust to future changes.
        \textbf{Rust also imposes mandatory braces}.

    \item Variables can be declared with the keyword \texttt{let} and type deduction rules inside the compiler avoids the need to specify a type in most cases. A variable name must be a valid identifier. Variable declaration with the same variable name \textbf{can't} appear twice or more in the same code block. Reusage of variables names within nested blocks are instead allowed, even with different types: \textbf{variable shadowing}. Currently the reference compiler does not emit any warning in case a variable is shadowed.

\end{itemize}

\clearpage
\section{DPCC compiler Implementation Details}

In this section it is briefly described the input stage, the lexer stage, and the syntax analysis stage (parser).
\textbf{Flex} and \textbf{Bison} are used as tools for aiding in the boilerplate code generation of respectively the lexer and the parser.

The compiler internally used 2 main types to model the source code of the input program: \texttt{token\_t}, \texttt{ast\_node\_t}.


\begin{itemize}
    \item \texttt{token\_t} is basically a book-keeping type. It is meant to store metadata for each token. The most fundamental
        fields that it stores are the \texttt{lexeme}, and the kind of each token (comment, identifier, string literal, \dots). It also stores the location of each token within the file \texttt{(line:column)}
    \item \texttt{ast\_node\_t} is instead the core type of the compiler. Multiple \texttt{ast\_node\_t}'s consitutes an AST node. When parsing
        with \textbf{Bison}, nodes are chained together in a parent/child relationship. Each node has the following fields:
        \begin{itemize}
            \item A pointer to a token.
            \item The node kind. The node kind is used to disambiguate the kind of the node (\texttt{Stmt, VarDeclStmt, Expr, \dots}). It's one of the most important fields used in the code generation phase.
            \item The codegen metadata. The codegen metadata is filled and used only in the last code generation phase of the compiler.
            \item A list of pointers to child nodes (if any).
            \item A backpointer to the parent node (if any).
            \item A pointer to the declaration node, used only for identifiers to lookup where they were declared.
            \item A literal value. The literal value is used only in literals to store the value represented by this node (\texttt{int, float, bool})
        \end{itemize}

    \item \texttt{symtable\_t} used to model the variables that are in scope. It's implementation for now is based on a linear array, and thus has linear search time performance.
        A hashmap could be used to improve the lookup performance.
    \item \texttt{ast\_traversal\_t} is a context state for traversing a fully expanded AST.
    \item \texttt{codegen\_ctx\_t} is a context state for tracking already used 3AC variable names.
\end{itemize}


\dots

Talk about common types: \texttt{ast\_node\_t}, \texttt{token\_t} and how they are mapped to the bison, flex equivalent.
How they are used and how they flow in the system. Explain that Bison is mostly used as a syntax
checker + symtable analysis, each node is defined and the pushed. Most complex type checking and
code generation steps are instead implemented somewhere else. Explain why you didn't implement
the necessary code right inside the bison grammar file.

\dots

\subsection{The input stage of the compiler}
This is the simplest part of the compiler. At the time of writing the \textbf{dpcc} compiler
allows only loading of files. In particular it reads an entire file into memory before continuining
with the rest of the piepeline. The input stage does not support URI, file downloads, any type of protocol
that would require realtime on stream code generation, and linux sockets. That is the
parser can open anything that looks like a file that has a finite determinable start, an end,
and a finite number of bytes.

\subsection{Lexer}

\urlref{https://github.com/westes/flex/}{Flex} is used to implement the lexer/tokenizer. Lexers are pretty simple to understand and
are particulary easy to develop if easing a tool like Flex. One can read the \urlref{https://www.cs.virginia.edu/~cr4bd/flex-manual/}{Flex Manual}.

The things that are worth noting about the \textbf{dpcc} tokenizer are:

\begin{itemize}
\item The lexer is completely UTF-8 aware, and UTF-8 symbols can be used to declare identifiers.UTF-8 aware and has particular
rules to match the variable encoding of UTF-8 \textbf{This allows variable names to include emojis}. Why? Cause it's cool \smiley
\item The lexer tracks line and column locations thanks to the \texttt{yylloc} exposed from bison. These
    variable is updated accordingly in the flex file whenever any token is encoutered the column information
    are updated and are resetted at each newline.
\item Support for C-style strings containing escape sequences.
\item Support for C-style single line comments
\item Support for binary and hexadecimal integers. Support for C-style floating point numbers with the exponent and an optional terminating \texttt{`f'} character.
\item All the remaining tokens are pretty standard and uninstering.
\end{itemize}

This module is mostly un-interesting. Flex is used mainly as a token recognizer since most of the logic is implemented outside the flex
file anyway. One of the most notable feature that is implemented outside the lex file and used,
is what's called \textbf{String interning}\footnote{\urlref{https://en.wikipedia.org/wiki/String_interning}{String Interning Wikipedia Article}}.
\textbf{String iterning} is a common technique used in compilers design that allows the compiler to
store and cache lexemes in a common place. Since in typical source files
the same lexemes tend to repeat and appear multiple times, it is common to store each lexeme only once. Whenever
a lexeme is found it is looked up in a string to string hashmap. If it's not found, it allocates
the new lexeme and returns the pointer to the new allocation. If it's found instead, it simply just returns the pointer to the interned lexeme. This allows to save
memory, but even more cooler is the fact that now if two strings are identical and they are interned correctly,
we can compare the two strings for equality by simply just comparing their respective pointers. In fact
thanks to string interning strings are uniquely identified by the memory address they live in.
This feature is then used in the parser
to quickly lookup the identifiers in the symbol table.

So upon encountering a new token the following things happen:

\begin{itemize}
\item It updates correctly the state required in order to track the location of each token in the source file.
\item The lexeme is interned, and the old lexeme pointer is stomped in favour of the interned one.
\item It allocates a new \texttt{token\_t} and set's up the lexeme pointer, it's location and some other metadata
\item It allocates a \texttt{ast\_node\_t} and attaches the token to it. The \texttt{ast\_node\_t} is what will be
    used by Bison and later stages to generate the AST and to perform the analysis and the code generation. The node
    kind is instead left un-initialized for now. It will be set by the \textbf{Bison} parser later, where more semantic context is available.
\item It signals to bison the new lexeme kind by simply \texttt{return}-ing from the \texttt{lex()} procedure (Bison calls flex in a coroutine mode)
\end{itemize}


\subsection{Parser}
The unfamiliar user can refer to the \urlref{https://www.gnu.org/software/bison/manual/bison.html}{Bison manual}.
The \textt{yylval} is aliased to be equivalent to \texttt{ast\_node\_t*}. The base case for a generation of an
\texttt{ast\_node\_t*} is handled inside \textbf{Flex}. In the implementation of \textbf{dpcc}, \textbf{Bison}
is merely used as a syntax analysis tool, and for resolving symbols declarations and/or invalid uses of undeclared symbols.
That means that the \textbf{Bison} file mostly defines the precedence of the operator and the grammar, while instead
the actions are pretty simple and mostly contain no code at all.

In fact almost all actions in the bison file, call some C functions defined somewhere else: \texttt{NEW\_NODE, push\_child, push\_childs}.
These C functions are used to create a new node and to set it's type, and to append the corresponding childs to this new node.
This keeps the bison file simple and concise since most of the heavyweight code is defined somewhere else.

The advantage of this method is: simplicity, abstraction, code decoupling, and code scaling. The disadvantage is that in order
to generate the code, at least 1 more AST pass is required, and thus could be potentially slower, than a single
pass compiler.

Also implementing concept logic directly inside the bison file, or implementing all the steps required for a modern
compiler (type deduction, type checking) in a single becomes quickly tedious at best, if not nearly impossible.

The \textbf{LAC} (Lookahead Correction) mechanism is enabled in the Bison file, since according to the Bison manual can provide
identification of the error location, and thus better error messages, at the cost of a very negligable runtime speed penalty.

The Ast marks each node with an \texttt{ast\_node\_kind\_t} so that we can disambiguate easier in later
stages what code should be generated for this node.

Also custom error reporting is used to replace the \textbf{Bison} default one (\texttt{yyreport\_syntax\_error}). The implemented custom error reporting
supports a GCC style error/warning and colored output and it it thus more user friendly.
One noticeable feature is that \textbf{dpcc} compiler can warn when a variable is declared but never used.

\subsection{Code Generation}

Order of the childs of each node \textbf{Matters} a lot.
The order of the childs in each node \textbf{is very important} and it drives
the correctness of the final output. In this way we can use simple printf to generate
the code each time we find something while traversing the tree

\subsection{Utilities and Misc}
\subsubsection{Custom logging}
Custom logging is implemented to override the default Bison logging, and is used also manually in the type checking
stage of the compiler to complain abot possible misuses.
Custom logging with colorized output and display of location where error occured, similar to \textbf{GCC}. Here it follows an example program that makes the \texttt{dpcc} compiler complain:
\begin{lstlisting}[language=DPL]
let array: int[2];
print(array[4]);
print(3.5 << 8);
\end{lstlisting}
    \begin{figure}[H]
        \centering
        \includegraphics[width=\linewidth]{imgs/log.png}
        \caption{\texttt{dpcc} custom logging showoff}
    \end{figure}

\subsubsection{Custom allocator wrapper}


Allows to clear the allocators at specific synchronization point in the compiler.
Given the nature of the short living time that this compiler is, this was probably not required

\subsubsection{Ast traversal functions}


\subsubsection{Typescript code generation}

Without going too further in the details, some part of the C source code composing the compilers are in fact
generated using a \textbf{Typescript} program. Now at the end this turned out to be moslty overkilled given
the size of this project, but nonetheless it was a cute little trick that I wanted to try and it was cool
after all.

Without spending too much time the \textbf{Typescript} program is used to do two things:

\begin{itemize}
    \item To generate the code for the type deduction and type checking of the each expression operator. It takes
        some \textbf{meta-representation} of what each expression accept as input types and which output type it produces.
        Given these \textbf{meta-representation} it generates a C function called \texttt{typecheck\_expr\_and\_operators} and some other utilities in a separate C file which is then compiled in the final executable.
        An example of such \textbf{meta-representation} is:
        \begin{lstlisting}[language=JS]
const MATH_EXPR = new Expr (
    [
        "ExprAdd", "ExprSub", "ExprMul", "ExprDiv", "ExprPow",
        "ExprInc", "ExprDec", "ExprPos", "ExprNeg",
    ],
    [
        new ExprTypeRule("int", ["int", "int"]),
        new ExprTypeRule("float", ["float", "int"]),
        new ExprTypeRule("float", ["int", "float"]),
        new ExprTypeRule("float", ["float", "float"]),

        new ExprTypeRule("int", ["int"]),
        new ExprTypeRule("float", ["float"]),
    ]
);
        \end{lstlisting}

    which rougly says that operators such as \texttt{+, -, *, /, **, ++, \dots} can either take integers or floats as inputs,
    and depending on which input types are provided, it either produces an \texttt{int} or \texttt{float} type as output.

    \item To embed the required preamble and postamble C code inside the \texttt{dpcc} executable. The preamble and
        postamble code that are outputted when calling \texttt{`./dpcc c <input>'} are in fact written
        into two separate files. The typescript program reads these two files and generate two header files
        containing two \texttt{uint8\_t[]} arrays that each encode the content of each respective file.
\end{itemize}




\subsection{Testing framework}


\section{Encountered problems}


\section{Performance results}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{imgs/dallrsz_performance_issue.png}
    \caption{Performance issue in \texttt{dallrsz} utility function}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics{imgs/dallrsz_code_usage.png}
    \caption{Performance issue in \texttt{dallrsz} utility function}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[scale=0.85]{imgs/dallrsz_callgraph.png}
    \caption{Performance issue in \texttt{dallrsz} utility function}
\end{figure}


\section{Conclusions}

...


\clearpage
\section{Appendix A: Structure of the Intermediate Code}
\label{appendix_a}

\section{Appendix: program text}

% Here you should include the program text.
% Do NOT use screenshots or similar methods.
% Below you can see how to use \lstinputlisting{}.



\begin{lstlisting}[language=DPL]
let a: int[] = 10;
let s = "Hello world";
{

}
\end{lstlisting}

\begin{lstlisting}[language=C]
int main() {

}
char **argv;
\end{lstlisting}


\clearpage
\section{Appendix B: Example Program Iterative Merge Sort}
\label{appendix_b}

\begin{lstlisting}[language=DPL]
let len = 32;

let array = [
    15, 59, 61, 75, 12, 71,  5, 35, 44,
    6, 98, 17, 81, 56, 53, 31, 20, 11,
    45, 80,  8, 34, 71, 83, 64, 28,  3,
    88, 50, 48, 80,  5
];


for (let cs = 1; cs < len; cs = 2 * cs) {
    for (let l = 0; l < len - 1; l = l + 2 * cs) {
        let m = len - 1;

        if ((l + cs - 1) < len - 1) {
            m = l + cs - 1;
        }

        let r = len - 1;

        if ((l + 2 * cs - 1) < len - 1) {
            r = l + 2 * cs - 1;
        }

        let n1 = m - l + 1;
        let n2 = r - m;

        let L: int[1024];
        let R: int[1024];

        for (let i = 0; i < n1; i++) {
            L[i] = array[l + i];
        }

        for (let i = 0; i < n2; i++) {
            R[i] = array[m + 1 + i];
        }


        let i = 0;
        let j = 0;
        let k = l;

        while (i < n1 && j < n2) {
            if (L[i] <= R[j]) {
                array[k++] = L[i++];
            } else {
                array[k++] = R[j++];
            }
        }

        while (i < n1) {
            array[k++] = L[i++];
        }
        while (j < n2) {
            array[k++] = R[j++];
        }
    }
}

print("Sorted array\n");
print(array);
\end{lstlisting}


% \section{Appendix: test cases} % Optional
%
% ...

% \section{Appendix: Extensions} % Optional
%
% ...

\end{document}
